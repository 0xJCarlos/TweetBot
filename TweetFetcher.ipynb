{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/0xJCarlos/TweetBot/blob/main/TweetFetcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TD5hisngQPJN"
   },
   "source": [
    "# Tweet Fetcher\n",
    "This is where the project begins, we need data to do everything in this project, and this is where we get that Data.\n",
    "\n",
    "We use the Tweepy library to get the tweets, the JSON library to manage the tweets to store them, and pandas to organize and store the tweets inside a DataFrame and then put them inside a .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yb1-jmEAq7k3",
    "outputId": "4c786721-b6f5-4506-8aed-8f8f3a7ab385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "#Step 0: Install Tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q4fXbB7EwyWk"
   },
   "outputs": [],
   "source": [
    "#Step 0.1: Import all the libraries into our project\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y8hT-OOArJgT"
   },
   "outputs": [],
   "source": [
    "#Step 0.2: Authentificate into the Twitter API with the Tweepy library\n",
    "\n",
    "#The Twitter API used in this project was deprecated after the purchase of Twitter by Elon Musk.\n",
    "#The consumer keys and access tokens in theory should not be valid anymore, but still were removed just in case.\n",
    "\n",
    "#consumer_key = \" \"\n",
    "#consumer_secret = \" \"\n",
    "#access_token = \" \"\n",
    "#access_token_secret = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "o7m0hcNj4M72"
   },
   "outputs": [],
   "source": [
    "#Step 1: Create the API's Objects\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) #Create the authentification object\n",
    "\n",
    "auth.set_access_token(access_token, access_token_secret ) #Configure the Access Token and Access Token Secret\n",
    "\n",
    "#Create the API Object passing the auth info, we also wait on rate limit because the API puts us on hold for some minutes when doing too many requests\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z1_ejrMZ1LLv"
   },
   "outputs": [],
   "source": [
    "#Step 2: Create the fetchTweets function to put the tweets inside a text file\n",
    "lista = [] \n",
    "def fetchTweet(tuits):\n",
    "    for each_tweet in tuits: \n",
    "      lista.append(each_tweet._json) #saves every tweet inside the list, in json format\n",
    "\n",
    "    with open('tweets_json.json','w') as file:  #writes the contents of the list as a json file\n",
    "      file.write(json.dumps(lista, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DNV0rviRgfF4"
   },
   "outputs": [],
   "source": [
    "#Step 3: Variables to request the last 2500 tweets from different accounts\n",
    "igntweets = tweepy.Cursor(api.user_timeline, screen_name = 'IGN', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "kotakutweets = tweepy.Cursor(api.user_timeline, screen_name = 'kotaku', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "polygontweets = tweepy.Cursor(api.user_timeline, screen_name = 'polygon', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "engadgettweets = tweepy.Cursor(api.user_timeline, screen_name = 'engadgetgaming', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "eurogamertweets = tweepy.Cursor(api.user_timeline, screen_name = 'eurogamer', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "gamesindustrytweets = tweepy.Cursor(api.user_timeline, screen_name = 'GIBiz', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "gamedevtweets = tweepy.Cursor(api.user_timeline, screen_name = 'gamedevdotcom', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "destructoidtweets = tweepy.Cursor(api.user_timeline, screen_name = 'destructoid', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "vg247tweets = tweepy.Cursor(api.user_timeline, screen_name = 'VG247', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "vgctweets = tweepy.Cursor(api.user_timeline, screen_name = 'VGC_News', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "gamesbeattweets = tweepy.Cursor(api.user_timeline, screen_name = 'GamesBeat', q='-filter:retweets', tweet_mode=\"extended\").items(2500)\n",
    "rockpapershottweets = tweepy.Cursor(api.user_timeline, screen_name = 'rockpapershot', q='-filter:retweets', tweet_mode=\"extended\").items(2500) \n",
    "\n",
    "accounts=[igntweets,kotakutweets,polygontweets,engadgettweets,eurogamertweets,gamesindustrytweets,gamedevtweets,destructoidtweets,vgctweets,vg247tweets,gamesbeattweets,rockpapershottweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZsSDfGu22DrM"
   },
   "outputs": [],
   "source": [
    "#Step 4: Fetch the tweets\n",
    "for tweet in accounts:\n",
    "  fetchTweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oogTCUzDM4ME"
   },
   "outputs": [],
   "source": [
    "#Step 5: read the .json file and convert to csv for further use.\n",
    "df = pd.read_json('/content/tweets_json.json') \n",
    "\n",
    "df.to_csv('tweets_raw.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaOs0gNdj3usqDXoIuI/ok",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TweetFetcher.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
